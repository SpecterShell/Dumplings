Version: 0.12.6
Installer:
- InstallerType: inno
  InstallerUrl: https://github.com/ollama/ollama/releases/download/v0.12.6/OllamaSetup.exe
Locale:
- Locale: en-US
  Key: ReleaseNotes
  Value: |-
    What's Changed
    - Flash attention is now enabled by default for Gemma 3, improving performance and memory utilization
    - Fixed issue where Ollama would hang while generating responses
    - Fixed issue where qwen3-coder would act in raw mode when using /api/generate or ollama run qwen3-coder <prompt>
    - Fixed qwen3-embedding providing invalid results
    - Ollama will now evict models correctly when num_gpu is set
    - Fixed issue where tool_index with a value of 0 would not be sent to the model
    Experimental Vulkan Support
    Experimental support for Vulkan is now available when you build locally from source. This will enable additional GPUs from AMD, and Intel which are not currently supported by Ollama. To build locally, install the Vulkan SDK and set VULKAN_SDK in your environment, then follow the developer instructions. In a future release, Vulkan support will be included in the binary release as well. Please file issues if you run into any problems.
    New Contributors
    - @yajianggroup made their first contribution in https://github.com/ollama/ollama/pull/12377
    - @inforithmics made their first contribution in https://github.com/ollama/ollama/pull/11835
    - @sbhavani made their first contribution in https://github.com/ollama/ollama/pull/12619
    Full Changelog: https://github.com/ollama/ollama/compare/v0.12.5...v0.12.6
- Key: ReleaseNotesUrl
  Value: https://github.com/ollama/ollama/releases/tag/v0.12.6
ReleaseTime: 2025-10-15T23:02:31.0000000Z
