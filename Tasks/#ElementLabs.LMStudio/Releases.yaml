0.3.2:
  ReleaseNotes: |-
    What's new in 0.3.2
    - New: Ability to pin models to the top is back!
      - Right-click on a model in My Models and select "Pin to top" to pin it to the top of the list.
    - Chat migration dialog now appears in the chat side bar.
      - You can migrate your chats from pre-0.3.0 versions from there.
      - As of v0.3.1, system prompts now migrated as well.
      - Your old chats are NOT deleted.
    - Don't show a badge with a number on the downloads button if there are no downloads
    - Added a button to collapse the FAQ sidebar in the Discover tab
    - Reduced default context size from 8K to 4K tokens to alleviate Out of Memory issues
      - You can still configure any context size you want in the model load settings
    - Added a warning next to the Flash Attention in model load settings
      - Flash Attention is experimental and may not be suitable for all models
    - Updated the bundled llama.cpp engine to 3246fe84d78c8ccccd4291132809236ef477e9ea (Aug 27)
    Bug Fixes
    - Bug fix: My Models model size aggregation was incorrect when you had multi-part model files
    - Bug fix: (Linux) RAG would fail due to missing bundled embedding model (fixed)
    - Bug fix: Flash Attention - KV cache quantization is back to FP16 by default
      - In 0.3.0, the K and V were both set to Q8, which introduced large latencies in some cases
        - You might notice an increase in memory consumption when FA is ON compared with 0.3.1, but on par with 0.2.31
    - Bug fix: On some setups app would hang at start up (fix + mitigation)
    - Bug fix: Fixed an issue where the downloads panel was dragged over the app top bar
    - Bug fix: Fixed typos in built-in code snippets (server tab)
0.3.3:
  ReleaseNotes: |-
    What's new in 0.3.3
    - Configuration Presets
      - Save your system prompt, inference parameters as a preset.
      - Each chat can be associated with a preset.
    - Show live token count for user input using the selected model's tokenizer
    - Show live token count for system prompt using the selected model's tokenizer
    - New buttons at the top of the chat: Clear All Messages, Clone Chat
    - Stats underneath input box: show context fullness % (click on the label to toggle to n_tokens/n_context)
    - QoL: Chat auto-name, skip if time to first token high or tok/sec low
    - Perf: switching between long chats should be faster
    - First model download suggestion: suggest Gemma 2B for machines with less than 12GB of VRAM
    - Copy debug info to clipboard by right clicking the gear icon next to the model
    - Server tab: Code Snippets now have its own top level tab
    - Right click on links shows a context menu with options to open in browser or copy link
    New settings
    - Open App Settings using [‚åò,] or [Ctrl + ,] from anywhere
      - Settings icon is now in the bottom right corner of the app, no longer in the sidebar
      - [‚åò4] or [Ctrl + 4] now opens My Models
    - ThemeSelector ([‚åòK ‚åòT] or [Ctrl + K, Ctrl + T] from anywhere)
    - Choose App update channel: Stable or Beta
      - Choose between Stable and Beta in the App Settings
      - Beta versions might have the same version number as Stable versions, but the build number will be higher
    - Settings option to disable double-click to edit a chat message
    Bug Fixes
    - Bug fix: [Windows] fix app close/minimize/maximize buttons background overflow
    - Bug fix: Helpful error when the first message is longer than the total context size
    - Bug fix: Button to show / hide intermediate steps in RAG sometimes did not work (fixed)
    - Bug fix: [Developer] Checkbox for showing debug info did not work when unchecked (fixed)
    - Bug fix: Open downloads folder will open the custom downloads folder instead of the default one
    - Bug fix: Updated Jinja template parser to support models using more complex templates
    - Bug fix: Esc wouldn't close some dialogs
    - Bug fix: on first app usage, can't create a chat until a model is loaded
    - Bug fix: downloads popover was covering the settings dialog when both open
    - Bug fix: cmd + R / ctrl + R to regenerate sometimes wouldn't work
    Migration from 0.2.31 Presets
    - Pre-0.3 presets are automatically migrated to the new format upon save. The old files are NOT deleted.
    - Load parameters are not currently included in the new preset format.
      - Favor editing the model's default config in My Models.
    Big thank you to community localizers üôè
    - Spanish @xtianpaiva
    - Norwegian @Exlo84
    - German @marcelMaier
    - Turkish @progesor
    - Russian @shelomitsky, @mlatysh, @Adjacentai
    - Korean @williamjeong2
    - Polish @danieltechdev
    - Czech @ladislavsulc
    - Vietnamese @trinhvanminh
    - Portuguese (BR) @Sm1g00l
    - Portuguese (PT) @catarino
    - Chinese (zh-HK), (zh-TW), (zh-CN) @neotan
    - Chinese (zh-Hant) @kywarai
    - Ukrainian (uk) @hmelenok
    - Japanese (ja) @digitalsp
0.3.4:
  ReleaseNotes: |-
    What's new in 0.3.4
    New
    - [Mac] Support for MLX! Read the announcement here
    - Mission Control: Cmd+Shift+M to search for models, Cmd+Shift+R to manage LM Runtimes
    - Set Structured Output (JSON Schema) from the UI
    - [UI] Move Chat Appearance control to top bar
    - [UI] Tweaks to size of per-message action buttons
    Bug Fixes
    - Fix for Black Screen after prolonged use (reference: lmstudio-bug-tracker#98)
    - Fix for no port other than 1234 working for the local server (reference: lms#80)
    - Fix for embedding API not working from Obsidian (reference: tracker#142)
    - Fix for RAG sometimes failing during document processing
0.3.5:
  ReleaseNotes: |-
    0. 3.5 - Release Notes
    Build 2
    - Fixes to in-app update when service mode is enabled
    - Fixes to JIT loading across client application lifetime
    Build 1
    - Run LM Studio as a service (headless)
      - lms load, lms server start no longer requires launching the GUI
      - ability to run on machine startup
    - Server start / stop button will remember last setting
      - This is useful when LM Studio is running as a service
    - Improvement to Model Search
      - Hugging Face search now happens automatically without Cmd / Ctrl + Enter
    - Just-In-Time model loading for OpenAI endpoints
    - Button to toggle Mission Control full screen / modal modes
    - Update llama.cpp-based JSON response generation; now supports more complex JSON schemas
    - Tray menu options to minimize app to tray, copy server base URL
    - Checkbox to add lms to PATH during onboarding on Linux
    - [Mac][MLX Vision] Bump mlx-vlm version to 0.0.15, support Qwen2VL
    - [Mac][MLX Engine] Updated Transformers to 4.45.0
      - Fixes some issues with sideloading quantized MLX models (https://github.com/lmstudio-ai/mlx-engine/issues/10)
    - [UI] Move Chat Appearance control to top bar
    - [UI] Tweaks to size of per-message action buttons
    - Localization:
      - Improved German translation thanks to Goekdeniz-Guelmez
      - Indonesian translation thanks to dwirx
    Bug fixes
    - [Bug fix] fix RAG reinjecting document into context on follow up prompts
    - Fixed RAG not working (https://github.com/lmstudio-ai/mlx-engine/issues/4)
    - Fix outline flicker around Mission Control
0.3.6:
  ReleaseNotes: |-
    0.3.6 - Release Notes
    LM Studio 0.3.6 introduces a Function Calling / Tool Use API through LM Studio's OpenAI compatibility API.
    This means you can use LM Studio with any framework that currently knows how to use OpenAI tools.
    We've also brought up a new and improved LM Studio in-app updater system.
    Tool Calling API (Beta)
    - New API: A drop-in replacement for OpenAI's Function Calling / Tool Use API
      - Supports both llama.cpp and MLX models
      - Supports both streaming and non-streaming
    - Tool use docs are available on: https://lmstudio.ai/docs/advanced/tool-use
    - Use any LLM that supports Tool Use and Function Calling through the OpenAI-like API
      - Qwen, Mistral, and Llama 3.1/3.2 models work well for tool use
    - Included starter code within the in-app code snippets
    - Improved tool call reliability through sampling configuration for both streaming and non-streaming tool use
    New vision input models
    - Added support for Qwen2VL (2B, 7B, 72B) and QVQ (72B) both in GGUF and MLX
    - Added support for .webp images
    - Added image auto resizing for vision models inputs, hardcoded to 500px width while keeping the aspect ratio
    New and improved installer
    - On Windows: you can now choose the installation drive and directory!
    - On all platforms: brand new LM Studio in-app updates system
      - Subscribe to either Stable or Beta updates. Beta updates will arrive more frequently.
        - Note: previously release channels did not work. Now they do.
    - App updates won't redownload 100s of MBs of dependencies that don't change and generally will be much smaller and faster to download.
      - And we've identified areas to make those even smaller
    - Progress bar when downloading in-app updates
    LM Runtimes improvements
    - Improved LM Runtime release system (llama.cpp, MLX, etc)
      - This was in place for a while, but we've automated nearly everything in this process.
    - In-app notifications when new llama.cpp / MLX versions are available to download
      - Will NOT require a full app update
    - Added "Missing Libraries" runtime compatibility status that allows users to "Fix" improperly installed runtimes
    UI improvements
    - Chat sidebar UI improvements
    - Server page UI improvements
    - Show release notes in-app after an update
    - Added a setting option to always use LLM to generate titles, even when generation speed is slow
    MLX updates (Apple Silicon)
    - Improved performance for MacOS 15
    - Improved performance for VLMs
    - Improved performance for long context generation
    - Bump mlx-engine dependencies versions: mlx==0.21.1, mlx_lm==0.20.4, mlx_vlm==0.1.4
    - Support for 3-bit and 6-bit quantization for MLX models
    Developer Experience
    - New in-app "Quick Docs" with code snippets and tool use examples (top right corner)
      - Opens in a new window for side-by-side usage in LM Studio
    - Improved error reporting in the server API
    - Pop out the server logs to a new window by pressing cmd / ctrl + shift + J
      - Works while in the chat page too
    - For power users: LM Studio's home directory moved from the historical ~/.cache/lm-studio to the new ~/.lmstudio.
      - This will only take effect for first-time installs.
      - If you already have data in ~/.cache/lm-studio the app will NOT attempt to move it.
    Bug fixes
    - Fixed "inputConfig required to render jinja prompt" when using the /completions API endpoint
    - Fixed first model download during onboarding showing blank screen
    - Fixed runtimes not being set up properly when app installed for All Users on Windows
    - Bugfix: make tool_call_id optional
    - Fixed a bug with jinja template processing for Qwen2VL and other models
    - Fixed a bug where newer LM Runtimes weren't selected after an update
    - Fixed clicking in chat name field while editing cancels the rename
    - [Mac] Fixed error when sending an image to vision enabled models
    - Fixed tool-call messages having "content": null are erroneously rejected
    - Fixed secondary click on presets sometimes not working
    - Fixed RAG not working
    - Fixed a bug where chats and folders couldn't be dragged to the root of the sidebar
    - Fixed a memory leak when counting tokens/using embedding models
    - Fixed installer being blurry on high DPI screens
    - Fixed when a new runtime is installed, the search filter for models is not updated
    - Fixed searching "Qwen VL" does not give correct results
    - Fixed a issue where models cannot be loaded when the app is still initializing
    - Fixed a issue where the chats page would softlock
    - Fixed a issue where the length of the scroll bar of the chat side bar would be out of sync with content height
    - Fixed runtime deletion not working
    - Fixed a rare bug will delete chats can sometimes softlock the app
    - Fixed lms load command
    - Fixed lms unload and lms status not working with embeddings
    - Fixed request logging when verbose logging is on (server)
    - Fixed some requests/responses are not redacted when logging prompts and responses are off
    - Fixed download resuming
    - Fixed more accessibility labels on UI elements
    - Fixed drag and drop file attachment not working
    - [MLX] Fixed bug which degraded performance for certain models
    - Fixed app not launching on some Linux distributions
    - Fixed accessibility button labels (previously showing Object object)
    - Fixed a bug where models without chat templates, including embedding models, could not be indexed
    - Fixed passing an empty or null tools array in the API request is treated as no tools provided
    - Fixed passing an empty or null tool_calls array in assistant messages in the API request is treated as the model making no cool calls
    - Fix for file attachments causing an error
    - Fix for long chat names pushing chat action buttons in the sidebar
    - Candidate fix for huggingface model search, download never reachable from within the app ("fetch failed")
    - Fix for pasting text from Microsoft Word giving an error about pasting an image
    - Fix for structured output for GGUF models lmstudio-bug-tracker/issues/173
    - Fix for machines with 1-2 CPU cores getting an error about CPU threads too low
    - Fixes to in-app update when service mode is enabled
    - Fixes to JIT loading across client application lifetime
0.3.20:
  ReleaseNotes: |-
    0.3.20 - Release Notes
    Build 3
    - Qwen3-Coder-480B-A35B tools support
    Build 2
    - Fixed a bug where the app would fail to open
    Build 1
    - Fix model loader flicker after selecting a model
0.3.21:
  ReleaseNotes: |-
    0.3.21 - Release Notes
    Build 4
    - Simplify Mission Control for User Mode
    Build 3
    - Parse Qwen3 coder tool calls that start with <function=... and not <tool_call>
    Build 2
    - Fix Qwen3-235B-A22B-Thinking-2507 thinking block handling
    - Fix Qwen3 coder 'tool_calls' emission in OpenAI-like /v1/chat/completions server
    - Introduce a Settings > Chat option to hide token counts from chat cells in the sidebar
    - Fix a bug where keyboard shortcut to chats page was lost after switching app modes
    - Avoid auto dismissing toasts if currently hovered over
    - Fix a bug where toggling message style would not reinitialize expandable messages
    - For new users, default to not showing chat token count in chat sidebar cells
    - Fixes 'Cannot apply filter "string" to type: NullValue' with Qwen3 Coder MOE models where null values were not being handled correctly in jinja parsing
    Build 1
    - Add confirmation toast after adding an MCP server
    - Show streaming preview for generating tool call arguments
    - By default, filter Staff Picks to models that can fit on your device
0.3.22:
  ReleaseNotes: |-
    0.3.22 - Release Notes
    New in LM Studio 0.3.22:
    - üéâ Support for OpenAI's gpt-oss models!
    - We partnered with OpenAI to bring their open-source models to LM Studio on launch day.
    - There are 2 models: gpt-oss-20b and gpt-oss-120b.
    - Learn more: https://lmstudio.ai/blog/gpt-oss
    Build 1
    - [gpt-oss] Bundle OpenAI/Harmony parsing framework on all platforms
    - [gpt-oss] More forgiving parsing of tool calls in chat
    - [gpt-oss] [OpenAI-compat API] Remove structural tokens (<|channel|>, <|end|>, etc...) from v1/chat/completions output
      - Should address issues seen with tools such as OpenCode
0.3.23:
  ReleaseNotes: |-
    0.3.23 - Release Notes
    Build 3
    - [llama.cpp][MoE] Add ability to offload expert weights to CPU/GPU RAM via "Force Model Expert Weights onto CPU" in advanced load settings
    - Tool names are normalized before being provided to the model (replace whitespaces, special chars)
    Build 2
    - Fix "Complete Download" button sometimes not working when downloading a staff-picked model
    - Fix "Fix" button not working for extension packs (like Harmony)
    - Fix "Cannot read properties of undefined (reading 'properties')" for certain tools-containing requests to /v1/chat/completions
    - Fix Error: EPERM: operation not permitted, unlink when auto-updating harmony
    Build 1
    - Bug fixes resulting in significant improvements for in chat tool calling reliability
    - Fixed a bug where some old conversations won't load in the app
    - Fixed a bug where tool call will fail sometimes when used via OpenAI compatible API in non-streaming mode
    - Fix models not outputting thinking tags in v1/chat/completions
      - For gpt-oss:
        - message.content will not include reasoning content or special tags
        - This matches the behavior of o3-mini.
        - Reasoning content will be in choices.message.reasoning (stream=false) and choices.delta.reasoning (stream=true)
    - Fix "TypeError: Invalid Version" causing app functionality issues on machines with AMD+NVIDIA GPUs
    - Fix bug where MCP plugin chip name was not rendering in User Mode
    - Fix bug where search results were refreshing on click
0.3.24:
  ReleaseNotes: |-
    0.3.24 - Release Notes
    New in LM Studio 0.3.24:
    - üéâ Support for ByteDance/Seed-OSS (36B)
    - Improved code blocks (sticky copy button) and markdown tables style
    - Improved lms (LM Studio CLI) command output style
    - Bug fixes
    Build 6
    - Fix a bug where markdown blocks could overflow beyond the chat screen
    - Fixed an issue with Seed-OSS where LM Studio did not parse tool calls for certain prompt templates
    Build 5
    - Support for ByteDance/Seed-OSS, including tool calling support and prompt template fixes
    - lms ps now provides more information about model status, context and TTL.
    - Fix a bug where pressing Tab did not correctly indent within the system prompt editor (Cmd/Ctrl + E)
    Build 4
    - Fixed a UI crash that would happen if the model being loaded was larger than the user's guardrails
    - Make lms (LM Studio CLI) output style more consistent
    Build 3
    - Fixed a bug where multiple config layers in model.yaml were in the wrong order
    - Added ability to edit chat name from chat tab dropdown, or double clicking chat tab
    Build 2
    - Fix sticky copy button bottom padding
    - Fix setting numCpuExpertLayersRatio via the SDK
    Build 1
    - Fix bug where context menu click on model loader would cause a flicker
    - Improved styling for tables in markdown
    - Improve code blocks: polished style and sticky copy code button
    - Fix bug where Harmony updates didn't respect the runtime extension pack "auto-update" setting
    - Adds Gaeilge translation, thanks to [@aindriu80] (https://github.com/aindriu80)
    - Updates to translations of Catalan, Chinese (Simplified), Chinese (Traditional), Japanese and Portuguese (Brazil).
    - Added app setting for default context length for all models. See under App Settings > General
    - Fixed a bug where setting context length to 0 would cause an error
    - Fixed a bug where setting values between (1.0, 1.1) would not work
    - Copying an assistant message now only copies the content, without formatting or thinking tokens
    - Fixed a bug where leftover speculative decoding setting would disable Markdown in chat
0.3.25:
  ReleaseNotes: |-
    0.3.25 - Release Notes
    New in LM Studio 0.3.25:
    - Select multiple chats in the sidebar for quick bulk actions (move, delete)
    - New option to move deleted chats to trash bin (or: instant delete)
    - NVIDIA Nemotron-Nano-v2 support
    Build 2
    - NVIDIA Nemotron-Nano-v2 support (incl. tool calling)
    Build 1
    - Select multiple chats and folders by holding Ctrl on PC or Cmd on Mac. Hold shift + click to select a range.
    - Option to move items to the operating system's trash bin instead of instantly deleting them
    - Improve app settings style and legibility
    - When selecting a chat that uses a different model, auto switch to that model if it's loaded
    - Bug fix: new chats and folders now correctly created relative to the selected folder
    - Chat: moved token counters to the app footer
    - Bug fix: ensure copy button appears on plaintext code blocks
0.3.26:
  ReleaseNotes: |-
    0.3.26 - Release Notes
    Build 6
    - The LM Studio CLI (lms) now supports streaming server logs, as well as model output.
      - Use lms log stream --source server for server logs
      - Use lms log stream --source model --filter input,output for both model input and output logs
      - Append --json to get JSON-formatted logs
      - Learn more: https://lmstudio.ai/docs/cli/log-stream
    - Fixed a bug where clicking Eject in the developer page would sometime open the configuration panel as well
    Build 5
    - [Windows] fixed a bug where Mission Control buttons were hard to click / behaved like a dragging surface
    Build 4
    - Fixed a bug where it was sometimes not possible to drag the app action bar when there was an image under it
    - New context menu 'Enclose in Folder' options when selecting multiple chats
    Build 3
    - [Linux] Fixed bug where rag-v1 did not work due to a missing embedding model
    - [UI] Switch to native context menus
    Build 2
    - Restored lms ls --detailed for backwards compatibility. Use lms ls or lms ls --json instead
    - Fixed bug where child processes were not cleaned up after LM Studio received SIGKILL
    Build 1
    - [MLX] Add badge for MXFP4 quantization type
0.3.27:
  ReleaseNotes: |-
    0.3.27 - Release Notes
    Build 4
    - Improved VRAM usage estimation, especially when flash attention is enabled
    Build 3
    - Setting to control whether to open the downloads panel after starting a model download (default: false)
    - Update CLI (lms) output colors to have better contrast in light mode
    - Fix a bug where copy buttons would sometimes not appear on conversation code blocks
    Build 2
    - New: Find in Chat (Cmd/Ctrl+F) and Search All Chats (Cmd/Ctrl+Shift+F).
    - New: Sort chats sidebar by date updated, date created, or token count
    - Model resources estimation will now work for vision models
    - Added model resources estimation to the CLI. You can now run lms load --estimate-only <model-name> to preview a model's estimated memory requirements before loading
    - While using lms chat, you can now use Ctrl^C to interrupt ongoing predictions
    Build 1
    - Additional model quantization files downloaded after the main model will now be properly nested under it
    - Improved memory usage estimation used for model loading guardrails.
      - Now memory estimation will take into account selected context lengths.
    - lms ps --json now reports model generation status and the number of queued prediction requests
0.3.28:
  ReleaseNotes: |-
    0.3.28 - Release Notes
    Build 2
    - Fix displaying model parameter counts in My Models and model picker
    - Improved Qwen format tool call reliability with llama.cpp engines
    - Fix UI theme resetting to 'Auto' after update to 0.3.28 (fixes issue in build 1)
    Build 1
    - New in My Models: ability to quickly select which model variant will be used by default
    - Significantly improved RAM/VRAM usage estimation for models that have mixed quantizations
    - Significantly improved RAM/VRAM usage estimation for models that have inaccurate parameter labels in their metadata
    - Significantly improved RAM/VRAM usage estimation for non-transformer models
    - Fixed a bug where models loaded via the CLI would not respect global GPU settings
    - Fix some flickering UI interactions on My Models page, and collapse indexer warnings under a new status button
    - Fix a bug where citations count badge with over 10 would clip text
    - Fix inconsistency in quantization badge text alignment
0.3.29:
  ReleaseNotes: |-
    0.3.29 - Release Notes
    Build 1
    - New OpenAI compatibility endpoint: /v1/responses
      - Create stateful interactions by passing the id of a previous responses as input to the next - no need to manage message history yourself
      - Custom tool calling support
      - Supports reasoning parsing, setting reasoning effort ("low"|"medium"|"high") for models like gpt-oss
      - Synchronous and streaming (stream=true)
      - See https://lmstudio.ai/docs/app/api/endpoints/openai for more details
    - New lms ls command option: lms ls --variants to list all variants for multi-variant models
0.3.30:
  ReleaseNotes: |-
    0.3.30 - Release Notes
    Build 1
    - Fixed streaming mode bug that was impacting tool calling functionality for Qwen 3 Coder in /v1/chat/completions API.
    - [Vulkan] Fix bug where models were not being loaded onto iGPUs (requires runtime update)
    - Compatibility support for the developer role in /v1/responses API endpoint. For now, developer messages will be processed as system messages internally.
0.3.31:
  ReleaseNotes: |-
    0.3.31 - Release Notes
    New in LM Studio 0.3.31:
    - Improve image understanding output quality, especially for OCR tasks
    - Default image attachment size is now 2048px on the longest side for better quality with vision models
      - Can be changed in Settings > Chat > Image Inputs
    - Support for MiniMax M2 tool calling
    - Flash Attention is now enabled by default for CUDA engines
    - New CLI commands to manage engines:
      - lms runtime get
      - lms runtime update
    - Improved macOS 26 compatibility and support
    Build 7
    - Fixed a bug where sometimes the Load Model button was greyed out for large MLX models
    - Set Flash Attention on by default for CUDA engines
    Build 6
    - Added tool use support for MiniMax M2
    - Added better controls for image input size for vision models
    Build 5
    - Update dependencies to support macOS 26
    - Server UX improvements
      - Log v1/chat/completions prompt processing progress to Developer Logs
      - Model-specific v1/chat/completions Developer Logs
      - Log error when JIT model load failed due to guardrails settings
    Build 4
    - Fixed "vision.imageResizeSettings key does not exist" error
    - Added settings for configuring maximum image input dimensions for vision models.
      - Accessible via Settings > Chat > Image Inputs > "Image resize bounds"
      - These settings control the maximum dimensions to which input images are resized before being sent to vision-capable models.
    Build 3
    - Added lms runtime get and lms runtime update CLI commands to manage runtime extensions from the terminal.
      - Run lms runtime -h for more info.
    - Fix issue where sometimes reasoning models' output would start with a plaintext <think> token
    Build 2
    - Increase image resize limits to 1024x1024 for improved image processing performance (superseded: now 2048px)
      - In a future update this will be user configurable (fixed in build 4)
    Build 1
    - [MLX] Fix ValueError: Image features and image tokens do not match for Qwen3 VL Thinking models
    - Fix occasional UI crash when searching models
0.3.32:
  ReleaseNotes: |-
    0.3.32 - Release Notes
    Build 2
    - Support for GLM 4.5 tool calling
    - [MLX] Fixed prompt template bug that caused GLM-4.1V to not recognize images
    - Support for olmOCR-2
    - Fix a bug where sometimes Download button would continue showing for an already downloaded model
    Build 1
    - Support for passing base64 images into OpenAI-compatible /v1/responses endpoint
      - See https://platform.openai.com/docs/guides/images-vision?api-mode=responses&format=base64-encoded#giving-a-model-images-as-input for details
    - Flash Attention is now enabled by default for Vulkan and Metal llama.cpp engines
    - Fix OpenAI-compatible /v1/responses endpoint "previous_response_not_found" bug due to internal file read error
    - Fix a bug where update toast would sometimes retrigger and close
    - Fix the "No model selected for this chat and no lastUsedModel recorded. Please select a model" error
    - Fixed cases where downloading additional variants of the same model sometime wouldn't get nested correctly
0.3.33:
  ReleaseNotes: |-
    0.3.33 - Release Notes
    Build 1
    - Support for MistralAI's Ministral models (3B, 8B, 13B)
    - Support for Olmo-3 tool calling
0.3.34:
  ReleaseNotes: |-
    0.3.34 - Release Notes
    Build 1
    - Support for EssentialAI's rnj-1 model
    - Fix jinja prompt formatting bug for some models where EOS tokens were not being included properly
0.3.35:
  ReleaseNotes: |-
    0.3.35 - Release Notes
    Build 1
    - [MLX] Support for Devstral-2 and GLM-4.6V
    - Fixed a bug where the default system prompt was still sent to the model even after the system prompt field was cleared.
    - Fixed a bug where exported chats did not include the correct system prompt.
    - Fixed a bug where the token count was incorrect when a default system prompt existed but the system prompt field was cleared.
    - Fixed a bug where sometimes the tool call results were not being added to the context correctly
0.3.36:
  ReleaseNotes: |-
    0.3.36 - Release Notes
    Build 1
    - FunctionGemma Support
0.3.37:
  ReleaseNotes: |-
    0.3.37 - Release Notes
    Build 1
    - Support for the LFM2 tool call format
    - Fix "Cannot read properties of null (reading 'architecture')" when using a generator
0.3.39:
  ReleaseNotes: |-
    0.3.39 - Release Notes
    Build 1
    - Support for image_url input in OpenAI-compatible v1/chat/completions REST endpoint
    - Support for top_logprobs in OpenAI-compatible v1/responses REST endpoint
    - Output cached_tokens statistics in OpenAI-compatible v1/responses REST endpoint
